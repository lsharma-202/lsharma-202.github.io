{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Hi, I'm Lokesh Sharma","text":"<p>Welcome to my personal portfolio! I'm passionate about knowledge graphs, machine learning, and building intelligent systems that make sense of data.</p>"},{"location":"contact/","title":"\ud83d\udcec Get in Touch","text":"<p>I\u2019d love to hear from you! Feel free to reach out via email or social media.</p>"},{"location":"contact/#email","title":"\ud83d\udce7 Email","text":"<ul> <li>lokesh@example.com</li> </ul>"},{"location":"contact/#social-media","title":"\ud83d\udd17 Social Media","text":"<ul> <li>LinkedIn</li> <li>GitHub</li> </ul>"},{"location":"contact/#get-in-touch_1","title":"\ud83d\udcec Get in Touch","text":"<p>I\u2019d love to hear from you! Please fill out the form below.</p> Name Email Message Send Message"},{"location":"resume/","title":"Resume","text":"<p>Resume</p>"},{"location":"blog/","title":"\ud83d\udcdd Blogposts","text":"<p>Welcome to my blog! Here, I share insights on knowledge graphs, machine learning, and more.</p>"},{"location":"blog/#latest-posts","title":"Latest Posts","text":""},{"location":"blog/#understanding-graph-neural-networks-with-pytorch-geometric","title":"Understanding Graph Neural Networks with PyTorch Geometric","text":"<p>Graph Neural Networks (GNNs) are a type of machine learning model designed to work with graph-structured data. They help capture relationships between data points by learning from the connections between them.</p>"},{"location":"blog/#ontologies-the-semantic-way-for-knowledge-representation","title":"Ontologies \u2014 The Semantic way for Knowledge Representation","text":"<p>We don\u2019t just store data \u2014 we connect ideas, define relationships, and build meaning. Ontologies provide the foundation to do this semantically, making knowledge both human-understandable and machine-readable.</p>"},{"location":"blog/#knowledge-graph-construction-with-ontologies-part-1","title":"Knowledge Graph Construction with Ontologies \u2014 Part 1","text":"<p>We\u2019ll use Prot\u00e9g\u00e9, a tool, to create a special structure \u2014 imagine it as a recipe for a MovieLens (EDA in my previous blog) dataset.</p>"},{"location":"blog/#older-posts","title":"Older Posts","text":"<p>View older posts \u2192</p>"},{"location":"blog/posts/","title":"Blog","text":""},{"location":"blog/posts/recommender/","title":"Why Graph Neural Networks are Powerful","text":"<p>Graph Neural Networks (GNNs) are a powerful tool for processing data that is naturally structured as graphs, such as social networks, molecular structures, and knowledge graphs.</p>"},{"location":"blog/posts/pyg/understanding_gnn/","title":"Understanding Graph Neural Networks with PyTorch Geometric","text":"<p>Graph Neural Networks (GNNs) are a type of machine learning model designed to work with graph-structured data. They help capture relationships between data points by learning from the connections between them.</p> <p>Why are GNNs important? Many real-world systems are naturally structured as graphs. Examples include social networks, the internet, molecular structures, and more. Even data types like text, images, and tables can be converted into graphs to uncover complex relationships.</p> <p>In this article, we'll go through the basics of graph data structures and how machine learning models can be built on top of them. Here's the roadmap:</p> <ul> <li> <p>Introduction to the Planetoid dataset and problem definition</p> </li> <li> <p>Overview of GNN architectures and key formulas</p> </li> <li> <p>Building PyTorch models using custom Python classes</p> </li> <li> <p>Training and evaluating the models</p> </li> <li> <p>Final thoughts and takeaways</p> </li> </ul>"},{"location":"blog/posts/pyg/understanding_gnn/#graph-datasets","title":"Graph Datasets","text":"<p>Graph datasets represent data as nodes (entities) and edges (relationships). Each node and edge can have its own features or attributes.</p> <p>We\u2019ll use the Planetoid package from PyTorch Geometric to work with pre-built graph datasets, reducing the amount of setup required.</p> <p>One commonly used dataset is Cora, a benchmark citation network. In Cora, each node represents a research paper, and edges represent citation links between papers. Each node has a feature vector based on the presence or absence of specific words in the paper (a total of 1433 possible words).</p> <p>Cora is widely used to test GNN models on tasks like node classification and link prediction.</p> <p></p>"},{"location":"blog/posts/pyg/understanding_gnn/#cora-dataset","title":"Cora Dataset","text":"<ul> <li> <p>x = [2708, 1433]: This is the node feature matrix. There are 2708 documents, each represented by a 1433-dimensional one-hot encoded feature vector.</p> </li> <li> <p>edge_index = [2, 10556]: This defines the graph structure. It shows how nodes are connected, with each column representing a directed edge.</p> </li> <li> <p>y = [2708]: These are the ground-truth labels. Each node belongs to exactly one class.</p> </li> <li> <p>train_mask, val_mask, test_mask (each of shape [2708]): These masks are used to split the nodes into training, validation, and test sets.</p> </li> </ul> <p>Let's pause for a moment to ponder. With a 1433-word feature vector, one could easily slap on a MLP model \ud83d\udc77 for some good old-fashioned node/document classification. But hey, we're not the ones to settle for the ordinary \ud83d\udd0e. We are going to cross the edge with edge_index to \ud83e\udd3e dive headfirst with those relationships to supercharge our predictions. So let's get seriously interconnected up in here! \ud83e\udd1d</p> <pre><code># Let us talk more about edge index/graph connectivity\nprint(f\"Shape of graph connectivity: {cora[0].edge_index.shape}\")\nprint(cora[0].edge_index)\n</code></pre> <p></p> <p>The edge_index is interesting as it contains two lists with the first list whispering the source node IDs, while the second spills the beans on their destinations. This setup has a fancy name: coordinate list (COO). It's a nifty way to efficiently store sparse matrices, like when you have nodes who aren't exactly chatty with everyone in the room. Now, I know what you're thinking. Why not use a simple adjacency matrix? Well, in the realm of graph data, not every node is a social butterfly. Those adjacency matrices? They would be swimming in a sea of zeros, and that's not the most memory-efficient setup. That's why COO is our go-to approach \ud83e\udde9 and PyG ensures the edges are inherently directed.</p> <pre><code># The adjacency matrix can be inferred from the edge_index with a utility function.\n\nadj_matrix = torch_geometric.utils.to_dense_adj(cora[0].edge_index)[0].numpy().astype(int)\nprint(f'Shape: {adj_matrix.shape}\\nAdjacency matrix: \\n{adj_matrix}')\n</code></pre> <pre><code># Some more PyG utility functions\nprint(f\"Directed: {cora[0].is_directed()}\")\nprint(f\"Isolated Nodes: {cora[0].has_isolated_nodes()}\")\nprint(f\"Has Self Loops: {cora[0].has_self_loops()}\")\n</code></pre> <p></p> <p>The Data object has many spectacular utility functions and let's give you a sneak peek with three examples: - is_directed tells if the graph is directed, i.e., the adjacency matrix is not symmetric. - has_isolated_edges sniffs out nodes that are loners, disconnected from the bustling crowd. These disconnected souls are like puzzle pieces without a complete picture, making downstream ML tasks a real head-scratcher. - has_self_loops informs if a node is in a relationship with itself \u2763</p> <p>Let's shortly talk about visualizations. Converting PyG Data objects into NetworkX graph objects and plotting them is a cake walk. But, hold your horses! Our guest list (number of nodes) is over 2k long, so an attempt to visualize it would be like squeezing a footballs stadium into your living room. Yeah, you don't want that \u26d4. So, while we are not participating in the plot parties, just know that this graph is primed and ready for some serious networking action, even if it's all happening behind the scenes \ud83c\udf10\ud83d\udd75\ufe0f\u200d\u2640\ufe0f</p>"},{"location":"blog/posts/pyg/understanding_gnn/#citeseer-dataset","title":"CiteSeer Dataset","text":"<p>CiteSeer is the scholarly \ud83c\udf93 sibling of Cora from the Platenoid family. It stands on the stage with 3,327 scientific papers with each node having exactly one of the 6 elite categories (class labels). Now, let's talk about data stats, where each paper/node in the CiteSeer universe is defined by a 3703-dimensional word vector with 0/1 values.</p> <p><code>citeseer = load_planetoid(name='CiteSeer')</code></p> <p></p> <pre><code>print(f\"Directed: {citeseer[0].is_directed()}\")\nprint(f\"Isolated Nodes: {citeseer[0].has_isolated_nodes()}\")\nprint(f\"Has Self Loops: {citeseer[0].has_self_loops()}\")\n</code></pre> <p></p> <p>With the citation network data duo that has already taken on the stage, we have got a slight twist in the scholarly saga. The CiteSeer dataset isn't all sunshine; it's got isolated nodes (remember our loners \u2753). Now classiffication task is gonna be slightly difficult with these guys in the game. Here's the catch: these isolated nodes pose a challenge to the GNNs aggregation (we will shortly talk about it) magic. We are limited to using only the feature vector representation for these isolated nodes, something that Multi-layer Perceptron (MLP) models do. The absence of adjacency matrix information might take its toll with a drop in accuracy. While we can't do much to fix this problem, we will do our best to shed light on the effect of their no-connectivity \ud83d\udcda\ud83d\udd0d.</p> <pre><code># Node degree distribution\n\nnode_degrees = torch_geometric.utils.degree(citeseer.edge_index[0]).numpy()\nnode_degrees = Counter(node_degrees)  # convertt to a dictionary object\n\n# Bar plot\nfig, ax = plt.subplots(figsize=(18, 6))\nax.set_xlabel('Node degree')\nax.set_ylabel('Number of nodes')\nax.set_title('CiteSeer - Node Degree Distribution')\nplt.bar(node_degrees.keys(),\n        node_degrees.values(),\n        color='#0A047A')\n</code></pre> <p></p> <p>CiteSeer has a majority of nodes with 1 or 2 neighbors. Now you might be thinking, \"What's the big deal?\". Well, let me tell you, it would be like hosting a party with just a couple of friends\u200a-\u200ait is cozy, but there is no rave. The global information about their connections to the community is gonna be lacking. This could be another challenge for GNNs when compared with Cora.</p>"},{"location":"blog/posts/pyg/understanding_gnn/#problem-definition","title":"Problem Definition","text":"<p>Our mission is now crystal clear: armed with node feature representations of each node and their connections to their neighboring nodes, we are on a quest to predict the correct class label for each and every node in the given graph.</p> <p>Note: we are not only relying on the surface-level node feature matrix but are diving deep into the data fabric, analyzing every interaction, and deciphering every whisper. Its more about understanding the dataset than make simple raw predictions based on patterns.</p>"},{"location":"blog/posts/pyg/understanding_gnn/#unravelling-graph-neural-networks","title":"Unravelling Graph Neural\u00a0Networks","text":"<p>We are about to demystify the magic behind GNNs. They represent nodes, edges, or graphs as numerical vectors such that each node resonates with its outgoing edges. But what's the secret sauce behind the GNNs? The technique of \"message passing, aggregation, and update\" operations. An analogy for this could be hosting a neighborhood block party, where each node aggregates information with its neighbors, transforms and updates itself, and then also share its updated insights with the rest of the crowd. It's about updating their feature vectors iteratively, infusing them with localized wisdom from their n-hop neighbors. Check out this gem: GNN Introduction which explains every concept with clarity.</p> <p>GNNs are made up of layers, with each layer extending its hop to access information from neighbors. For example, a GNN with 2 layers a node would consider friend-of-firend distance to collect insights and update its representation.</p>"},{"location":"blog/posts/pyg/understanding_gnn/#basic-gnn","title":"Basic GNN","text":"<p>We are creating a base class that lays the groundwork for our actual GNN models.We are also setting up private methods to initialize animation-related statistics. The base class will be inherited later by GCN and GAT models to tap into the shared functionalities without fuss. Effortless efficiency is right at your fingertips \ud83d\udee0\ufe0f\ud83d\udcca\ud83c\udfd7\ufe0f.</p> <pre><code>class BaseGNN(torch.nn.Module):\n    \"\"\"\n    Base class for Graph Neural Network models.\n    \"\"\"\n\n    def __init__(\n        self,\n    ):\n        super().__init__()\n        torch.manual_seed(48)\n        # Initialize lists to store animation-related statistics\n        self._init_animate_stats()\n        self.optimizer = None\n\n    def _init_animate_stats(self) -&gt; None:\n        \"\"\"Initialize animation-related statistics.\"\"\"\n        self.embeddings = []\n        self.losses = []\n        self.train_accuracies = []\n        self.val_accuracies = []\n        self.predictions = []\n\n    def _update_animate_stats(\n        self,\n        embedding: torch.Tensor,\n        loss: torch.Tensor,\n        train_accuracy: float,\n        val_accuracy: float,\n        prediction: torch.Tensor,\n    ) -&gt; None:\n        # Update animation-related statistics with new data\n        self.embeddings.append(embedding)\n        self.losses.append(loss)\n        self.train_accuracies.append(train_accuracy)\n        self.val_accuracies.append(val_accuracy)\n        self.predictions.append(prediction)\n\n    def accuracy(self, pred_y: torch.Tensor, y: torch.Tensor) -&gt; float:\n        \"\"\"\n        Calculate accuracy between predicted and true labels.\n\n        :param pred (torch.Tensor): Predicted labels.\n        :param y (torch.Tensor): True labels.\n\n        :returns: Accuracy value.\n        \"\"\"\n        return ((pred_y == y).sum() / len(y)).item()\n\n    def fit(self, data: Data, epochs: int) -&gt; None:\n        \"\"\"\n        Train the GNN model on the provided data.\n\n        :param data: The dataset to use for training.\n        :param epochs: Number of training epochs.\n        \"\"\"\n        # Use CrossEntropyLoss as the criterion for training\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = self.optimizer\n\n        self.train()\n        for epoch in range(epochs + 1):\n            # Training\n            optimizer.zero_grad()\n            _, out = self(data.x, data.edge_index)\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            acc = self.accuracy(\n                out[data.train_mask].argmax(dim=1), data.y[data.train_mask]\n            )\n            loss.backward()\n            optimizer.step()\n\n            # Validation\n            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n            val_acc = self.accuracy(\n                out[data.val_mask].argmax(dim=1), data.y[data.val_mask]\n            )\n            kwargs = {\n                \"embedding\": out.detach().cpu().numpy(),\n                \"loss\": loss.detach().cpu().numpy(),\n                \"train_accuracy\": acc,\n                \"val_accuracy\": val_acc,\n                \"prediction\": out.argmax(dim=1).detach().cpu().numpy(),\n            }\n\n            # Update animation-related statistics\n            self._update_animate_stats(**kwargs)\n            # Print metrics every 10 epochs\n            if epoch % 25 == 0:\n                print(\n                    f\"Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: \"\n                    f\"{acc * 100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | \"\n                    f\"Val Acc: {val_acc * 100:.2f}%\"\n                )\n\n    @torch.no_grad()\n    def test(self, data: Data) -&gt; float:\n        \"\"\"\n        Evaluate the model on the test set and return the accuracy score.\n\n        :param data: The dataset to use for testing.\n        :return: Test accuracy.\n        \"\"\"\n        # Set the model to evaluation mode\n        self.eval()\n        _, out = self(data.x, data.edge_index)\n        acc = self.accuracy(\n            out.argmax(dim=1)[data.test_mask], data.y[data.test_mask]\n        )\n        return acc\n</code></pre>"},{"location":"blog/posts/pyg/understanding_gnn/#multi-layer-perceptron-network","title":"Multi-Layer Perceptron Network","text":"<p>Here comes the vanilla Multi-layer Perceptron Network! In theory, we could predict a document/node's category just by looking at its features. No need for relational info\u200a-\u200ajust the good old bag-of-words representation. To test the hypothesis, we define a simple 2-layer MLP that works solely with input node features.</p>"},{"location":"blog/posts/pyg/understanding_gnn/#graph-convolutional-networks","title":"Graph Convolutional Networks","text":"<p>Convolutional Neural Networks (CNNs) have taken the ML scene by storm, thanks to their neat parameter-sharing trick and the ability to efficiently extract latent features. But aren't images also graphs? Confused! Let's think of each pixel as a node and the RGB values as node features. So a question emerges: can these CNN tricks be pulled off in the realm of irregular graphs?</p> <p>It's not as simple as copy-pasting. Graphs have their own quirks: * Lack of Consistency: Flexibility's great, but it brings some chaos. Just think of molecules with the same formula but different structures. Graphs can be tricky like that. * Node-Order Mystery: Graphs have no fixed order, unlike texts or images. A node's like a guest at a party\u200a-\u200ano set place. Algorithms need to be chill \ud83d\udd73 about this lack of node hierarchy. * Scaling Woes: Graphs can grow BIG. Imagine social networks with billions of users and trillions of edges. Operating on that scale isn't a walk in the park.</p> <pre><code>class GCN(BaseGNN):\n    \"\"\"\n    Graph Convolutional Network model for node classification.\n    \"\"\"\n\n    def __init__(\n        self, input_dim: int, hidden_dim: int, output_dim: int\n    ):\n        super().__init__()\n        self.gcn1 = GCNConv(input_dim, hidden_dim)\n        self.gcn2 = GCNConv(hidden_dim, output_dim)\n        self.optimizer = torch.optim.Adam(\n            self.parameters(), lr=0.01, weight_decay=5e-4\n        )\n\n    def forward(\n        self, x: torch.Tensor, edge_index: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the Graph Convolutional Network model.\n\n        :param (torch.Tensor): Input feature tensor.\n        :param (torch.Tensor): Graph connectivity information\n        :returns torch.Tensor: Output tensor.\n        \"\"\"\n        h = F.dropout(x, p=0.5, training=self.training)\n        h = self.gcn1(h, edge_index).relu()\n        h = F.dropout(h, p=0.5, training=self.training)\n        h = self.gcn2(h, edge_index)\n\n       return h, F.log_softmax(h, dim=1)\n</code></pre> <pre><code>---\n\nclass GAT(BaseGNN):\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,\n                 heads: int=8):\n        super().__init__()\n        torch.manual_seed(48)\n        self.gcn1 = GATConv(input_dim, hidden_dim, heads=heads)\n        self.gcn2 = GATConv(hidden_dim * heads, output_dim, heads=1)\n        self.optimizer = torch.optim.Adam(\n            self.parameters(), lr=0.01, weight_decay=5e-4\n        )\n\n    def forward(\n            self, x: torch.Tensor, edge_index: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the Graph Convolutional Network model.\n\n        :param (torch.Tensor): Input feature tensor.\n        :param (torch.Tensor): Graph connectivity information\n        :returns torch.Tensor: Output tensor.\n        \"\"\"\n        h = F.dropout(x, p=0.6, training=self.training)\n        h = self.gcn1(h, edge_index).relu()\n        h = F.dropout(h, p=0.6, training=self.training)\n        h = self.gcn2(h, edge_index).relu()\n        return h, F.log_softmax(h, dim=1)\n</code></pre>"},{"location":"blog/posts/pyg/understanding_gnn/#model-training","title":"Model Training","text":"<p>Let us see how the latent representations of the nodes in the graph evolve over time, as the model undergoes training for node classification tasks.</p> <pre><code>num_epochs = 200\ndef train_and_test_model(model, data: Data, num_epochs: int) -&gt; tuple:\n    \"\"\"\n    Train and test a given model on the provided data.\n\n    :param model: The PyTorch model to train and test.\n    :param data: The dataset to use for training and testing.\n    :param num_epochs: Number of training epochs.\n    :return: A tuple containing the trained model and the test accuracy.\n    \"\"\"\n    model.fit(data, num_epochs)\n    test_acc = model.test(data)\n    return model, test_acc\n\nmlp = MLP(\n    input_dim=cora.num_features,\n    hidden_dim=16,\n    out_dim=cora.num_classes,\n)\nprint(f\"{mlp}\\n\", f\"-\"*88)\nmlp, test_acc_mlp = train_and_test_model(mlp, data, num_epochs)\nprint(f\"-\"*88)\nprint(f\"\\nTest accuracy: {test_acc_mlp * 100:.2f}%\\n\")\n</code></pre> <p></p> <p></p> <p>As one can see, our MLP seems to be struggling in the spotlight, with just around 55% test accuracy. But why does the MLP do not perform better? The main culprit is none other than overfitting\u200a-\u200athe model's become too cozy with the training data, leaving it clueless when facing new node representations. Its like predicting labels with one eye closed. It also fails to incorporate an important bias into the model. That is exactly where GNNs come into play and can help to boost the performance of our model.</p> <pre><code>gcn = GCN(\n    input_dim=cora.num_features,\n    hidden_dim=16,\n    output_dim=cora.num_classes,\n)\nprint(f\"{gcn}\\n\", f\"-\"*88)\ngcn, test_acc_gcn = train_and_test_model(gcn, data, num_epochs)\nprint(f\"-\"*88)\nprint(f\"\\nTest accuracy: {test_acc_gcn * 100:.2f}%\\n\")\n</code></pre> <p> </p> <p>And there you have it\u200a-\u200aby just swapping out those linear layers GCN layers, we're skyrocketing to a dazzling 79% test accuracy!\u2728 A testament to the power of relational information between nodes. It's like we've turned on a data spotlight, revealing hidden patterns and connections that were previously lost in the shadows. The numbers don't lie\u200a-\u200aGNNs aren't just algorithms; they're data whisperers.</p> <p>Similarly, even GATs perform with a higher accuracy (81%) due to their multi-headed attention feature.</p> <pre><code>gat = GAT(\n    input_dim=cora.num_features,\n    hidden_dim=8,\n    output_dim=cora.num_classes,\n    heads=6,\n)\nprint(f\"{gat}\\n\", f\"-\"*88)\ngat, test_acc_gat = train_and_test_model(gat, data, num_epochs)\nprint(f\"-\"*88)\nprint(f\"\\nTest accuracy: {test_acc_gat * 100:.2f}%\\n\")\n</code></pre> <p> </p> <p></p> <p>Let us look at the latent representation of our CiteSeer dataset using the TSNE dimensionality reduction technique. We use <code>matplotlib</code> and <code>seaborn</code> for plotting the nodes of the graph.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\n# Get embeddings\nembeddings, _ = gat(citeseer[0].x, citeseer[0].edge_index)\n\n# Train TSNE\ntsne = TSNE(n_components=2, learning_rate='auto',\n            init='pca').fit_transform(embeddings.detach())\n\n# Set the Seaborn theme\nsns.set_theme(style=\"whitegrid\")\n\n# Plot TSNE\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nsns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=data.y, palette=\"viridis\", s=50)\nplt.legend([], [], frameon=False)\nplt.show()\n</code></pre> <p></p> <p>The data canvas paints a revealing picture: nodes of the same class gravitate towards each other forming clusters for each of the six class labels. However, outliers isolated nodes have a role in this drama, as they have introduced a twist to our accuracy scores.</p> <p>Remember our inital guess about the impact of mising edges? Well the hypothesis has its say. We are putting through another test, where I aim to calculate performance of GAT model by calculating the accuracy categorized by node degrees, thus revealing the importance of connectivity.</p> <p></p>"},{"location":"blog/posts/pyg/understanding_gnn/#wrapping-up","title":"Wrapping up","text":"<p>And with that, we reach the final section, I like to summarize the key takeaways: 1. We have seen why GNNs outshine MLPs and underscored the pivotal role of node relations. 2. GATs often outperform GCNs due to self-attention's dynamic weights, leading to better embeddings. 3. Be cautious with layer stacking; too many layers can lead to over-smoothing, where embeddings converge and lose diversity.</p>"},{"location":"blog/posts/sw/kg_construction/","title":"Knowledge Graph Construction with Ontologies \u2014 Part 1","text":"<p>Confused by terms like \u201cKnowledge Graphs,\u201d \u201cGraph Databases,\u201d \u201cSemantic Web,\u201d or \u201cOntologies\u201d? Don\u2019t worry, this blog post is here to help! I get the overwhelm from these tech terms. Questions like \u201cWhat are these?\u201d \u201cWhen to use them?\u201d \u201cWhy do they matter?\u201d and \u201cHow do ontologies link to knowledge graphs?\u201d can be puzzling. But fear not! I\u2019ll explain in simple terms, making it accessible to all. Let\u2019s explore ontologies and their importance. Plus, I\u2019ll show you how Python can automatically turn flat csv files into a knowledge graph using a set ontology \u2014 it\u2019s like adding contacts to your phone. Exciting, right? \ud83d\ude80\ud83d\udcda\ud83d\udd0d</p> <p></p> <p>We\u2019ll use Prot\u00e9g\u00e9, a tool, to create a special structure \u2014 imagine it as a recipe for a MovieLens (EDA in my previous blog) dataset. Then, enter Python \u2014 we\u2019re getting smart here. We\u2019ll give the computer simple directions, so it can make a knowledge graph from this recipe. It\u2019s like teaching a dog a trick, but Python is the clever one.</p> <p>But hold on, what\u2019s the reason behind all this effort? Imagine having a huge jigsaw puzzle, and your aim is to piece it together for the full picture. Crafting graphs from data is a bit like that \u2014 it helps us organize a ton of information during the data ingestion process. And ontologies? They provide structure and rules, making sure the graph makes perfect sense. Picture this: you\u2019re tidying up a messy room \u2014 you put labels on different things like books, toys, and clothes. Ontologies do a similar job, but for data. They neatly categorize information and show how these categories are connected. \ud83d\udccc\ud83c\udf10</p> <p>We aim to maximize automation in the data ingestion process when constructing knowledge graphs, particularly when dealing with extensive data and diverse organizational information. Manual tasks can be exhausting and error prone. Let\u2019s promptly clarify some terms. This will establish a strong base, particularly for those new to this field. \ud83d\udc49</p>"},{"location":"blog/posts/sw/kg_construction/#understanding-ontologies","title":"Understanding Ontologies","text":"<p>In the context of the semantic web and knowledge graphs, an ontology serves as a formal description of concepts, connections, properties, and rules within a specific domain. These components are often visually represented using a graph model, where nodes symbolize concepts or classes, and connections illustrate the relationships between these nodes. For example, in the field of medicine, nodes could include \u2018diseases,\u2019 \u2018symptoms,\u2019 and \u2018patients,\u2019 while connections might portray relationships like \u2018causes,\u2019 \u2018treats,\u2019 and \u2018diagnoses.\u2019 Think of ontologies as intricate maps that systematically arranges complex real-world elements into a coherent guide that computers can understand.</p> <p>To put it even simpler, ontologies transform the complexities of reality into a structured guide that helps computers understand and categorize knowledge about real-world objects in an abstract manner. To construct such a description, we meticulously define various components: individuals (object instances), classes, attributes, relations, as well as constraints, rules, and axioms governing them. This methodical approach not only offers a shared and reusable representation of knowledge but also has the potential to expand our domain understanding with new insights.</p> <p>Let us construct a basic ontology onto for our MovieLens dataset employing Prot\u00e9g\u00e9.</p> <p></p> <p>We export the onto in a compact and human-readable Turtle format. The syntack of Turtle is widely used to represent RDF data and provides meaningful prefixes and is a convenient choice for exchanging semantic information.</p> <p></p> <p>For users who prefer coding and want a more streamlined approach to ontology conversion, \u201cOwlready2\u201d offers a better fit. Its automation and targeted output formats align with a coding-centric workflow, making it a suitable choice for those who want to integrate ontology concepts efficiently into software projects without the complexities of a comprehensive tool like Protege. \u2728</p> <p>Parsing the ontology in Python The RDFlib is a popular Python library with tools and utilities to create, manipulate, and query RDF data and ontologies. The below code snippet demonstrates to load and parse ontology stored in Turtle format.</p> <pre><code>def _parseOntology(path: str = None) -&gt; rdflib.Graph():\n    import rdflib\n    # Instantiate a graph as defined in the rdflib library\n    onto = rdflib.Graph()\n    onto.parse(path, format='turtle')\n    return onto\n</code></pre> <p>Next, we define two cypher queries to read the classes and their associated datatype properties and ranges. The queries are structured using RDF and OWL prefixes, selecting distinct classes and aggregating property-type pairs for each class using SPARQL. At last the result is grouped by class</p> <pre><code># read the onto and generate cypher\nclasses_and_props_query = \"\"\"\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n\nSELECT DISTINCT ?class (GROUP_CONCAT(DISTINCT ?propTypePair ; SEPARATOR=\",\") AS ?props)\nWHERE {\n    ?class rdf:type owl:Class .\n    optional {\n      ?prop rdfs:domain ?class ;\n        a owl:DatatypeProperty ;\n        rdfs:range ?range .\n      BIND (concat(str(?prop),';',str(?range)) AS ?propTypePair)\n    }\n  } GROUP BY ?class  \"\"\"\n</code></pre> <p>The query selects distinct relationships, along with their domains and ranges. The resulting triples are filtered by property class types such as ObjectProperty and FunctionalProperty. The final query retrieves relationships, domains, and ranges for analysis or processing.</p> <pre><code>relations_query = \"\"\"\nprefix owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n\nSELECT DISTINCT ?rel ?dom ?ran #(GROUP_CONCAT(DISTINCT ?relTriplet ; SEPARATOR=\",\") AS ?rels)\nWHERE {\n    ?rel a ?propertyClass .\n    filter(?propertyClass in (rdf:Property, owl:ObjectProperty, owl:FunctionalProperty, owl:AsymmetricProperty,\n           owl:InverseFunctionalProperty, owl:IrreflexiveProperty, owl:ReflexiveProperty, owl:SymmetricProperty, owl:TransitiveProperty))\n\n    ?rel rdfs:domain ?dom ;\n      rdfs:range ?ran .\n\n    #BIND (concat(str(?rel),';',str(?dom),';',str(?range)) AS ?relTriplet)\n\n  }\"\"\"\n</code></pre>"},{"location":"blog/posts/sw/kg_construction/#the-significance-of-ontologies","title":"The Significance of Ontologies","text":"<p>You might be wondering, why do we even need ontologies? Well, they bridge the gap between human and machine understanding, enabling meaningful interactions. Even more crucially, they design adaptable data structures, essential for capturing the complexities of our data-rich and IoT-driven world. Ontologies go beyond connecting humans and computers; they also unify organizations. Imagine everyone utilizing ontologies \u2014 data management becomes seamless, reducing complexities and fueling insights. These ontologies establish a universal language where rules prevent confusion, elucidate concepts, and facilitate automated processes and scalable data handling. In today\u2019s interconnected world, efficient collaboration is essential for shaping a better future, wouldn\u2019t you agree? \ud83e\udd16</p> <p>Key components of an ontology include: \ud83d\udcdd\ud83d\udc49</p> <ol> <li>Classes/Concepts: The main characters \u2014 think of them as categories like \u201cPatient,\u201d \u201cDoctor,\u201d and \u201cDisease.\u201d They\u2019re the building blocks that give our playset structure.</li> <li>Individuals/Instances: Meet the specific players, like \u201cDr. Strange\u201d \u2014 they\u2019re the actual characters who make our story real. They fit into the Classes/Concepts we\u2019ve set up.</li> <li>Properties: These are like relationships between characters, showing how they\u2019re linked. For instance, the \u201chasSymptom\u201d property links \u201cDisease\u201d and \u201cSymptom,\u201d just like clues in a detective story.</li> <li>Axioms and Constraints: These are like the rules of our world. Axioms are like detective\u2019s notes \u2014 logical statements that unravel relationships. And constraints are like stage directions, guiding the characters\u2019 actions.</li> <li>Hierarchies and Taxonomies: Imagine arranging characters in a family tree \u2014 the big categories are like the elders, and the specific ones are like their kids. It\u2019s like giving order to our cast.</li> <li>Inference: This is where the magic happens \u2014 characters\u2019 actions lead to conclusions. It\u2019s like predicting a twist in the plot based on the clues we\u2019ve uncovered.</li> </ol> <p></p> <p>Notable ontology languages encompass RDF, OWL, and SKOS. These languages empower various domains to establish standardized ontologies for exchanging and annotating domain-specific data. Experts within each domain primarily oversee ontology development, which can stem from sources such as schema.org, industry specifications, or even be created from the ground up.</p> <p>Do schemas and ontologies share the same definition? \ud83d\udd0d No, an ontology acts as an adaptable blueprint for mutual domain comprehension, while a schema is an inflexible blueprint for organizing data within a particular system. Ontologies permit flexible expansions, commonly in AI and knowledge management, while schemas guarantee data consistency in database systems. \ud83c\udf1f</p>"},{"location":"blog/posts/sw/kg_construction/#crucial-vocabularies-come-into-play","title":"Crucial vocabularies come into play:","text":"<ul> <li>RDF: short for Resource Description Framework, is the web\u2019s data maestro, working behind the scenes in search engines, online shopping recommendations, and social media feeds. Wherever data needs to be understood and connected, RDF quietly operates. It forms the bedrock for modelling and exchanging web data, represented as triples \u2014 subject-predicate-object statements. Within an RDF, the triple elements could take the form of resources identified by unique resource identifiers (URIs, well-known as URLs), literals (akin to those in XML), or auxiliary blank spaces.</li> <li>OWL: short for Web Ontology Language, is a prominent tool for constructing web-based ontologies. It delves deeper than RDF by introducing constructs that define classes, properties, and relationships. This enables developers to move beyond hierarchies and incorporate complex relationships and rules, making OWL a potent asset for representing and sharing knowledge in the semantic web landscape.</li> <li>GraphDB redefines data storage by capturing nodes and relationships rather than conventional tables or documents. They excel over traditional SQL databases in handling intricate relationships without complex JOIN operations, and also surpass many NoSQL databases in providing intuitive and efficient querying of highly connected data. Neo4j, an open-source, ACID-compliant graph database, is a key player. It\u2019s available as AuraDB\u2019s managed service or self-hosted with Community or Enterprise Editions. \ud83d\ude80</li> <li>GraphQL: Similar to how SQL assists in querying data from conventional databases, GraphQL plays a comparable role, but for databases organized uniquely using RDF. While it mirrors SQL\u2019s functionality, GraphQL\u2019s focus lies in connecting diverse data fragments, rather than traditional rows and columns. If you\u2019re well-versed in SQL querying, envision GraphQL as the tailored tool for a distinct data structure. Notably, options include SPARQL for most graph databases, Cypher for Neo4j, Gremlin under the Apache TinkerPop framework, and more. Your choice of graph database and query language hinges on your project\u2019s specifics and the underlying data model.</li> </ul> <p>In this part, we covered ontologies, knowledge graphs, and graph models, exploring their relationships and dependencies. We discussed ontologies\u2019 role in building scalable, diverse data graphs between organizations. We created an ontology in Protege and parsed it with Cypher queries. We also explored additional terminology. The blog part \u2014 2 will demonstrate how our ontology generates a knowledge graph from CSV files automatically, offering a practical illustration. \ud83c\udf1f</p>"},{"location":"blog/posts/sw/ontology_intro/","title":"OntologiesThe Semantic way for Knowledge Representation","text":"<p>Welcome to the world of Semantic Web! \ud83c\udf10 It's a broad term where ideas and technologies come together to tackle the overwhelming amount of information scattered across the internet. The goal? Standardising information stored in disparate data silos Making data structured and interoperable to allow inferencing Up-float knowledge that is not explicitly stated, even for machines.</p> <p></p> <p>The overwhelming amount of information scattered across the\u00a0internetIf you believe in above goals or been overwhelmed by different terminologies, this article will simplify the chaotic landscape of the Semantic Web. Here is what you can expect: - Introduction - The need for Semantic Web - Understanding Resource Description Frameworks (RDF) - Why do we care about RDFS and Shared Vocabularies - Knowledge Representation as Ontologies</p> <p>But first, let's talk about the everyday struggles we face in the non-semantic (HTML-based or similar) web world.</p>"},{"location":"blog/posts/sw/ontology_intro/#the-search-struggle-misinformation-social-media-flood-and-generative-ai-mayhem","title":"The Search Struggle: Misinformation, Social Media Flood, and Generative AI\u00a0Mayhem","text":"<p>Sure, search engines are lightning-fast \u26a1, but are they always spot-on? \ud83c\udfaf Nope! We still end up sifting through a mountain of irrelevant search results just to find what we're looking for, the misinformation maze and the unstoppable flood \ud83c\udf0a of social media content. Adding to the chaos, we've got Generative AI bots churning out content faster than we can say 'Semantic Web'! \ud83d\ude05 These challenges not only diminish the quality of information but also consume precious time. The need for the Semantic Web is evident in addressing these challenges.</p> <p>So, how does it work its magic? Well, picture this: every concept and the relationship between these concepts on the web gets its own special ID badge called a Universal Resource Identifier (a string of characters uniquely identifies any resource, where a \"resource\" can be anything from a \"thing\" to a \"concept\"). We have already known them for some time with our web page URLs, but let's not get confused here. So far these URLs were pointing at documents \ud83c\udf9f\ufe0f (the HTML web pages), what I am talking about is URIs pointing to each atomic concept present within a single document (a HTML web page).</p> <p>As you can guess just with URI we are able to loosen much of the ambiguity in the information. But wait, there's more! Semantic Web takes a step further by asking these concepts to hang out in fancy hierarchies of classificationscalled ontologies \ud83d\udc6a. Now we can search smarter with our required classified keywords of concepts and relationships, thanks to these URIs and ontologies. It's like having a super-smart librarian who knows exactly where to find what you're looking for. \ud83d\udcda Let us continue by looking at some key components in this realm</p>"},{"location":"blog/posts/sw/ontology_intro/#resource-description-framework-rdf","title":"Resource Description Framework (RDF)","text":"<p>We have all seen how Generative AI goes off the rails when creating contentthe hallucinations is due to its lack of ability to truly grasp the context/meaning behind the information it is processing. And this not only affects the accuracy of search results but also hinders our progress toward a more semantic web. To bridge this gap, we need two things: 1. A way to uniquely identify every concept 2. A universal system for conveying and interpreting their meaning</p> <p>But, as with any great endeavour, there are hurdles to overcome. One such challenge is achieving consensus on the definitions of these concepts and ensuring seamless data sharing. Sounds daunting, right? So, what's the plan? Well, let's kick things off by making data interoperable within organisations. Think of like laying the foundation for a grand data-sharing party, complete with invitations, guest lists, and maybe even some fancy dance moves (data governance, policies, access controlyou name it! \ud83d\udc83\ud83d\udd7a).</p> <p>One strategy in traction is the concept of \"meta-tagging\" resources. It's like adding little sticky notes to our data, providing additional context in the form of properties and values. For example, we could tag \"Harper Lee\" as the author of \"To Kill a Mockingbird\" and throw in some extra info like gender (let's say female, just for kicks).</p> <p>With this base the World Wide Web Consortium developed Resource Description Framework (RDF) for automated semantic processing of information. The RDF structures information into neat little statements called triples, each consisting of a subject, predicate, and object \ud83d\udcca. One can think of it as a data-model for humans and a language for machines to speakone where they can declare how concepts are connected to each other and what property values they have. Now, let's talk about these tripleseach triple is made up of three atomic elements, represented by URIs and could be one of the following: - a link to a resource - a link to a property definition - a literal value (like \"33\") - a blank node (a mysterious placeholder)</p> <p>And there are two popular models for organising triples:\u00a0 - the classic subject-predicate-object and\u00a0 - the more casual entity-attribute-value</p> <ul> <li>To illustrate, let's revisit our buddy Harper Lee and her age-defying literary masterpiece. We can express the fact that \"The book <code>To Kill a Mockingbird</code> was written by Harper Lee, who was born in 1926\" in a machine-readable format (RDF).</li> </ul> <pre><code>Triple 1 &lt;Subject-Predicate-Object &gt;:\nSubject: The book 'To Kill a Mockingbird'\nPredicate: writtenBy\nObject: Harper Lee\n\nTriple 2 &lt;entity-attribute-value&gt;:\nSubject: Harper Lee\nPredicate: born\nObject: 1926\n\n# To express these statements in RDF triples:\nTriple 1:\nSubject (URI): http://example.com/books/to-kill-a-mockingbird\nPredicate (URI): http://example.com/properties/written-by\nObject (URI): http://example.com/authors/harper-lee\n\nTriple 2:\nSubject (URI): http://example.com/authors/harper-lee\nPredicate (URI): http://example.com/properties/born\nObject (literal): 1926\n</code></pre> <p>Let's define a namespace <code>ex</code> for the base URI <code>http://example.com/</code> for readability and let's use the Turtle format representation to express the same RDF information.</p> <pre><code>@prefix ex: &lt;http://example.com/&gt;.\nex:books/to-kill-a-mockingbird ex:properties/written-by ex:authors/harper-lee .\nex:authors/harper-lee ex:properties/born 1926 .\n</code></pre> <p>It's like giving our data a passport for the semantic webneat, organized, and ready for adventure. Great! but isn't this too tedious? Restructuring each piece of information into triples and then figuring out how to leverage them to address our scalability concerns!\ud83e\uddd0</p> <p>Let's bring Named Graphs into our discussion \ud83e\udd13a set of triples named by identified by a unique name or identifier (URI). This URI can later be used outside or with the scope of the graph to refer to it</p> <p>Named Graphs lets us compartmentalise RDF data into distinct, named contexts or graphs, kind of like putting them into labeled folders. This makes data management a breeze, allowing us to scale up, track versions, enforce security, and even throw in some provenance tracking for good measure. So, there you have itthe journey from chaotic data to organized knowledge, guided by RDF, triples, and named graphs \ud83c\udf1f</p>"},{"location":"blog/posts/sw/ontology_intro/#rdf-schema-rdfs","title":"RDF Schema\u00a0(RDFS)","text":"<p>Ah, so we've got RDF, our trusty model for representing data with our own vocabularies. But wait, there's more! While RDF lays down the groundwork, it doesn't quite cover all the bases when it comes to expressing the nitty-gritty of complex relationships and constraints in our data. RDF doesn't play favorites with any particular application domain or set the semantics for how things should be understood within a domain. Nope, that's left entirely up to us, and this is where RDF Schema (RDFS) comes into play.</p> <p></p> <p>RDF Schema: A Vocabulary Description LanguageRDFS extends RDF's capabilities, acting as a vocabulary description language for describing properties and classes with flair. While, RDF provides a basic framework for representing data, RDFS enhances it with additional features for defining schemas, specifying constraints, and enabling multi-hop inferencing \u2192 leading to semantically richer data models. The resources in the RDFS vocabulary also have URIs with the prefix\u00a0:</p> <p><code>@prefix rdfs: http://www.w3.org/2000/01/rdf-schema#</code></p> <p>RDF Schema is a primitive ontology language that offers certain modelling advantages like: 1. Hierarchical Structure: Creation of relationships between <code>classes</code> and <code>sub-classes</code> which enhances the expressiveness of RDF by allowing for more complex classification and inferencing 2. Domain and Range Constraints: enables specifications for properties and restrictions on the allowed types of resources that can be linked to a specific property. This helps in ensuring data consistency and validity. 3. Property Inheritance: Properties defined within a class are automatically inherited by its sub-classes. This reduces redundancy and simplifies the ontology by allowing the reuse of existing properties. 4. Inference and Reasoning: enables automated reasoning engines to derive additional information from existing data within the domain -specific constraints</p> <p>Now, let's dive into the RDFS vocabulary and its key components: 1. Classes: A class can be thought of as a container of related items, with each item inside the container referred to as an instance of that class. If it helps, you can think of a class as a broad category or type, similar to how objects are grouped in object-oriented programming languages. RDFS classes can represent a wide range of things, such as web pages, people, documents, databases, or abstract ideas. These classes are defined using special terms provided by RDF Schema, like rdfs:Class and rdfs:Resource, and are linked using properties like rdf:type and rdfs:subClassOf. The connection between instances and classes is established using the rdf:type property. 2. Properties: Besides classes, properties are essential for adding characteristics to these classes of entities. For instance, think of 'numberOfPages' in a Book class. In RDFS, properties are defined using the RDF class rdf:Property and associated with RDFS properties such as rdfs:domain, rdfs:range, and rdfs:subPropertyOf</p> <p>Keep in mind: Just as in programming languages, typing is employed to ensure proper usage of objects within a class. In RDF documents, the use of classes places constraints on which objects properties can be assigned to, establishing permissible domains or ranges for specific properties.</p> <p>Now, let's simplify all this with a little example:</p> <p>The <code>rdfs:range</code> property tells us which class the values of a specific property belong to. For instance, if we want to say that the ex:author property's values are instances of the ex:Person class, we use these RDF statements:</p> <pre><code>ex:Person rdf:type rdfs:Class .\nex:author rdf:type rdf:Property .\nex:author rdfs:range ex:Person .\n</code></pre> <p>These statements clarify that <code>ex:Person</code> represents a class, ex:author is a property, and any RDF statements involving the ex:author property will have instances of ex:Person as their values. On the other hand, the rdfs:domain property indicates which class the property is associated with. For example, if we want to specify that the ex:author property pertains to instances of the ex:Book class, we use these RDF statements:</p> <pre><code>ex:Book rdf:type rdfs:Class .\nex:author rdf:type rdf:Property .\nex:author rdfs:domain ex:Book .\n</code></pre> <p>These statements clarify that ex:Book represents a class, ex:author is a property, and any RDF statements involving the ex:author property will have instances of ex:Book as their subjects.</p>"},{"location":"blog/posts/sw/ontology_intro/#shared-vocabularies","title":"Shared vocabularies","text":"<p>RDFS enables the creation of custom vocabularies, but isn't it wise to laverage an existing vocabulary crafted by others who believes in the potential of semantic web and are defining vocabularies a similar conceptual domain? These publicly accessible vocabularies, known as shared vocabularies not only prove cost-effective by also promote the shared comprehension of the described domains.</p> <p></p> <p>The notion of shared vocabularies is pivotal in realizing the Semantic Web's objectives, as it allows different data sources to communicate effectively with each other and to be integrated into a unified knowledge graph. By employing shared vocabularies, developers and organizations can ensure the easy comprehension, interpretation and fusion of their datawith that from other sources.</p> <p>Examples include: 1. Dublin Core Metadata Initiative (DCMI): Offers standardized metadata terms for describing resources like documents, images, and web pages, covering crucial attributes such as title, author, date, and subject, enhancing resource discovery and management on the web. 2. Friend of a Friend (FOAF): A vocabulary for portraying people and their relationships in machine-readable format, with terms for personal profiles, social networks, and connections, enabling seamless exchange of user and social network data across applications. 3. Schema.org: A collaborative venture by major search engines to establish a shared vocabulary for describing structured web data, with terms encompassing events, products, organizations, and creative works, ensuring easy comprehension by search engines and applications. 4. Geospatial Vocabularies: Various vocabularies like GeoNames ontology, Simple Features Vocabulary, and GeoSPARQL vocabulary for geospatial data representation, defining terms for geographical features, coordinates, and spatial relationships, fostering interoperability across systems. 5. SKOS (Simple Knowledge Organization System): A vocabulary for representing knowledge organization systems like taxonomies and thesauri, offering terms for concepts, labels, and relationships, facilitating structured knowledge resource management and exchange.</p>"},{"location":"blog/posts/sw/ontology_intro/#_1","title":"OntologiesThe Semantic way for Knowledge Representation","text":""},{"location":"blog/posts/sw/ontology_intro/#knowledge-representation-as-ontologies","title":"Knowledge Representation as Ontologies","text":"<p>In the field of artificial intelligence (AI), Knowledge Representation serves as a common modelling approach, utilising mathematical logics such as first-order logic. Ontologies are a specific framework within this paradigm, incorporating logical formalisms like OWL (Web Ontology Language) to structure and organize information. They formally describe a domain of related concepts and their relationships.</p> <p></p> <p>Ontologies can encompass the concepts and structures we saw in RDF, RDFS, and shared vocabularies, but they often go beyond them by including additional elements such as axioms, rules, and logical expressions to capture more complex knowledge structures and facilitate reasoning. They are important because they serve as semantic schemata (or 'intelligent' views over information resources) for automated reasoning about the data. An ontology O can be formally represented as a quadruple: O = , where: <ul> <li>C: a set of classes representing concepts within the domain under consideration. For example, \"Author,\" \"Book,\" \"Genre,\" and \"Publisher.\"</li> <li>R: a set of relations, also known as properties or predicates, establishing connections between instances of classes. For example, the relation \"Author wrote Book\" indicates the association between an author and a book.</li> <li>I: a set of instances, each of which may belong to one or more classes and may be linked to other instances or literal values through relations. For example, an instance might be \"Harper Lee,\" belonging to the class \"Author,\" and linked to the instance \"To Kill a Mockingbird\" via the \"wrote\" relation.</li> <li>A: a set of axioms, which define logical rules or constraints within the ontology. For example, an axiom might specify that if an author was born before 1930, they are considered a pre-World War II author.</li> </ul> <p>In this configuration, the ontology facilitates the structured representation of knowledge within the domain, enabling precise modeling of relationships between entities such as authors and books and supporting reasoning over the data. If we are to classify ontologies:</p> <ol> <li>Light-weight vs. Heavy-weight Ontologies: Light-weight: These ontologies are simpler and more scalable, suitable for efficient reasoning. Example: FOAF (Friend of a Friend) ontology.</li> <li>Heavy-weight: These ontologies are more complex and offer higher predictive power but may require more computational resources. Example: SUMO (Suggested Upper Merged Ontology).</li> <li>Upper-level, Domain, and Application Ontologies: Upper-level: Provide general knowledge applicable across various domains. Example: Cyc.       Domain: Focus on specific subject areas like healthcare or finance. Example: SNOMED CT (Systematized Nomenclature of MedicineClinical Terms).       Application: Tailored to specific software applications or industries. Example: MusicBrainz ontology.       3Schema-ontologies:       Describe classes of objects, properties, and relationships similar to database schemas. Example: Dublin Core Metadata Element Set.</li> <li>Topic-ontologies:    Define hierarchies of topics or subjects for classification purposes. Example: Library of Congress Subject Headings (LCSH).</li> <li>Lexical Ontologies: Provide formal semantics for lexical concepts, like word meanings and relationships. Example: WordNet.</li> </ol> <p></p> <p>Now, let's introduce one final term in this article: Knowledge Bases. While ontologies define the structure and semantics of knowledge, Knowledge Bases store and manage the actual instances of that knowledge. Picture ontologies as the blueprint or schema for organising knowledge, while knowledge bases act as the repositories where the actual data resides.</p> <p>Here are some examples of knowledge bases:</p> <ol> <li>Wikidata: A free and open knowledge base containing data on a wide array of topics, including people, places, organizations, events, and concepts. \ud83c\udf10</li> <li>DBpedia: A knowledge base that extracts structured information from Wikipedia and presents it as Linked Data. \ud83d\udd17</li> <li>Freebase (No Longer Active): A collaborative knowledge base and data repository acquired by Google in 2010. \ud83d\udd70\ufe0f</li> <li>YAGO (Yet Another Great Ontology): A large knowledge base that amalgamates information from Wikipedia, WordNet, and other sources. \ud83d\udcda</li> <li>Google Knowledge Graph: A knowledge base employed by Google to enrich search results and furnish users with pertinent information. \ud83d\udd0d</li> </ol> <p>That concludes our discussion for today. I trust this article provided you with a comprehensive introduction to the evolution of the semantic web, its significance, and the key components essential to embark on your semantic journey. There's a wealth of resources available, so feel free to share more in the comments section to foster the linking of information via common entities. \u2728</p>"},{"location":"projects/project1/","title":"KG-based Recommender System","text":"<p>Using knowledge graphs and graph neural networks, this project recommends related entities intelligently.</p>"},{"location":"projects/project1/#tech-stack","title":"Tech Stack","text":"<ul> <li>Python</li> <li>PyG</li> <li>PyKEEN</li> <li>RDFLib</li> </ul> <p>\ud83d\udc49 Next</p>"},{"location":"projects/project2/","title":"Project 2","text":""}]}